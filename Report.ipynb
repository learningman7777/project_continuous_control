{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2. Continuous Control\n",
    "I trained double-jointed arm agent to move to target locations(Green Sphere) using DDPG(Deep Deterministic Policy Gradient, Kind of Actor-Critic Method).\n",
    "<img src=\"image/trained GIF.gif\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "given double-jointed arm agent and moving Green Sphere. \n",
    "\n",
    "#### State\n",
    "The observation space consists of 33 variables corresponding to position, rotation, velocity, and angular velocities of the arm.\n",
    "\n",
    "#### Action\n",
    "Each action is a vector with four numbers, corresponding to torque applicable to two joints. Every entry in the action vector should be a number between -1 and 1.\n",
    "\n",
    "#### Reward\n",
    "A reward of +0.1 is provided for each step that the agent's hand is in the goal location.\n",
    "\n",
    "#### Goal\n",
    "agents must get an average score of +30 over 100 consecutive episodes\n",
    "\n",
    "\n",
    "## Algorithms\n",
    "\n",
    "#### Experience Replay\n",
    "When the agent interacts with the environment, the sequence of experience tuples can be highly correlated. To break harmful correlations, algorithm contains a collection of experience tuples and samples a small batch of tuples in order to learn. it's experience replay.\n",
    "\n",
    "#### Actor-Critic Method\n",
    "* actor:takes in a state and outputs the distribution over actions.\n",
    "* the critic takes in a state and outputs a state value function of policy Pi, V Pi.\n",
    "\n",
    "#### DDPG (Deep Deterministic Policy Gradient)\n",
    "DDPG is a different kind of actor-critic method.  \n",
    "DDPG use two deep neural networks. the actor and the critic.  \n",
    "the actor is used to approximate the optimal policy deterministically, not stochastically. That means we want to always output the best believed action for any given state.  \n",
    "The critic learns to evaluate the optimal action value function by using the actors best believed action.  \n",
    "**pseudocode**  \n",
    "<img src=\"image/DDPGPaper.png\">\n",
    "\n",
    "## Model Architecture\n",
    "\n",
    "#### Network Layer\n",
    "###### Actor\n",
    "1. input state\n",
    "2. HiddenLayer128(relu)\n",
    "3. HiddenLayer128(relu)\n",
    "4. output action(tanh)\n",
    "\n",
    "###### Critic\n",
    "1. input state\n",
    "2. HiddenLayer128(relu)\n",
    "3. HiddenLayer128(relu)\n",
    "4. output value of state\n",
    "\n",
    "\n",
    "#### Hyper Parameters\n",
    "1. Replay Buffer Size = 20000\n",
    "2. Mini Batch Size = 128        \n",
    "3. Gamma(discount factor) = 0.99\n",
    "4. Actor Learning Rate = 0.0001 \n",
    "5. Critic Learning Rate = 0.001\n",
    "6. TAU(for soft update of target parameters) = 0.001\n",
    "7. n_episodes = 1000\n",
    "8. UPDATE_EVERY = 20\n",
    "9. ITERATION_AT_UPDATE = 10\n",
    "\n",
    "## Plot of Rewards\n",
    "<img src=\"image/result.png\">\n",
    "\n",
    "## Ideas for Future Work\n",
    "* Crawl : i want to challenge Crawl project in unity-ML-Agents later. it has more difficult continuous control environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
